{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74defa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer,StandardScaler\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf5b80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Encoded_House.csv')\n",
    "\n",
    "x=df.drop(['log_price','price'],axis=1).values\n",
    "y=df['log_price'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e284266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== Test Train Split======\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#====== Standerization ======\n",
    "scaler_x = StandardScaler()\n",
    "x_train = scaler_x.fit_transform(x_train)\n",
    "x_test = scaler_x.transform(x_test)\n",
    "\n",
    "#====== Adding bias ======\n",
    "x_train_bias=np.c_[x_train, np.ones((x_train.shape[0]))]\n",
    "x_test_bias=np.c_[x_test, np.ones((x_test.shape[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530f286",
   "metadata": {},
   "source": [
    "### OLS Method\n",
    "β = (XᵀX)⁻¹Xᵀy , To minimize error, we differentiate Error = ||Xβ-Y||^2 with respect to β, and make it equal to 0, thus get β\n",
    "\n",
    "If two or more columns in X_train_bias are linearly dependent, then XᵀX become singular, Use the Moore-Penrose pseudo-inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f586cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.linalg.pinv(np.dot(x_train_bias.T,x_train_bias)) \n",
    "B=np.dot(x_train_bias.T,y_train)\n",
    "theta_ols=np.dot(A,B)\n",
    "\n",
    "#=== Prediction ===\n",
    "y_pred_ols=np.dot(x_test_bias,theta_ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fcb50",
   "metadata": {},
   "source": [
    "### Gradient Decent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73ffa2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, lr=0.05, threshold=1e-7, max_iter=100000): \n",
    "    m = x.shape[0]\n",
    "    theta = np.zeros((x.shape[1], 1))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        pred = x @ theta\n",
    "        cost = (1 / (2 * m)) * np.sum((pred - y) ** 2)\n",
    "        if cost < threshold:\n",
    "            break\n",
    "\n",
    "        gradients = (1 / m) * x.T @ (pred - y)\n",
    "        theta -= lr * gradients\n",
    "\n",
    "    return theta\n",
    "\n",
    "#=== Prediction ===\n",
    "theta_gd = gradient_descent(x_train_bias, y_train, lr=0.05)\n",
    "y_pred_gd = x_test_bias @ theta_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee33e94",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff448b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(x, y, alpha=0.1, lr=0.05, threshold=1e-7, max_iter=100000): \n",
    "    m = x.shape[0]\n",
    "    theta = np.zeros((x.shape[1], 1))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        pred = x @ theta\n",
    "        cost = (1 / (2 * m)) * np.sum((pred - y) ** 2) + (alpha / (2 * m)) * np.sum(theta ** 2)\n",
    "        if cost < threshold:\n",
    "            break\n",
    "\n",
    "        gradients = (1 / m) * x.T @ (pred - y) + (alpha / m) * theta\n",
    "        theta -= lr * gradients\n",
    "\n",
    "    return theta\n",
    "\n",
    "#=== Prediction ===\n",
    "theta_gd_ridge = ridge_regression(x_train_bias, y_train, lr=0.01)\n",
    "y_pred_gd_ridge = x_test_bias @ theta_gd_ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aeadf6",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6be1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(x, y, alpha=0.01, lr=0.05, threshold=1e-7, max_iter=100000): \n",
    "    m = x.shape[0]\n",
    "    theta = np.zeros((x.shape[1], 1))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        pred = x @ theta\n",
    "        cost = (1 / (2 * m)) * np.sum((pred - y) ** 2) + alpha * np.sum(abs(theta))\n",
    "        if cost < threshold:\n",
    "            break\n",
    "\n",
    "        gradients = (1 / m) * x.T @ (pred - y) + alpha*np.sign(theta)\n",
    "        theta -= lr * gradients\n",
    "\n",
    "    return theta\n",
    "\n",
    "#=== Prediction ===\n",
    "theta_gd_lasso = lasso_regression(x_train_bias, y_train, lr=0.01)\n",
    "y_pred_gd_lasso = x_test_bias @ theta_gd_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35c8e1",
   "metadata": {},
   "source": [
    "#### Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e388118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Regression [ R²: 0.601611, RMSE: 0.45249, MAE: 0.36889 ]\n",
      "Gradient Descent Regression [ R²: 0.601611, RMSE: 0.45249, MAE: 0.36889 ]\n",
      "Ridge Regression [ R²: 0.601670, RMSE: 0.45245, MAE: 0.36887 ]\n",
      "Lasso Regression [ R²: 0.592474, RMSE: 0.45765, MAE: 0.36995 ]\n"
     ]
    }
   ],
   "source": [
    "# ====== Evalution ======\n",
    "def evaluate(y_true, y_pred, label=\"Model\"):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"{label} [ R²: {r2:.6f}, RMSE: {rmse:.5f}, MAE: {mae:.5f} ]\")\n",
    "\n",
    "evaluate(y_test, y_pred_ols, \"OLS Regression\")\n",
    "evaluate(y_test, y_pred_gd, \"Gradient Descent Regression\")\n",
    "evaluate(y_test, y_pred_gd_ridge, \"Ridge Regression\")\n",
    "evaluate(y_test, y_pred_gd_lasso, \"Lasso Regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
